#!/usr/bin/env python3
"""
Semantic Search + Knowledge Graph Demo
======================================

This script demonstrates the complete semantic search and knowledge graph
capabilities of the system, including:

1. Semantic Document Search (meaning-based, not keywords)
2. Entity Extraction with LangGraph
3. Knowledge Graph Storage in NebulaGraph
4. Graph Traversal for Indirect Connections
5. Pattern Discovery

Example Use Cases:
- "Why are shipments getting delayed?" â†’ Finds port congestion, weather events
- "Which companies affected by Mumbai port?" â†’ Graph traversal
- "Find incidents similar to cyber attacks" â†’ Pattern matching
"""
import os, sys
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'utils', '.env'))

import json
from typing import List, Dict, Any

# Sample documents for demonstration
SAMPLE_DOCUMENTS = [
    {
        "id": "doc_001",
        "text": """Company X faced shipment delays due to heavy rainfall near Mumbai port. 
        The logistics bottleneck affected multiple suppliers in the region. 
        Port congestion has been reported for the past two weeks, causing 
        customs clearance issues for imported cargo.""",
        "category": "supply_chain"
    },
    {
        "id": "doc_002", 
        "text": """Weather related supply chain disruption: Heavy monsoon rainfall 
        caused flooding near the port facilities. Supplier B reported inability 
        to deliver raw materials to Company A. The delay is expected to last 
        3-5 business days.""",
        "category": "supply_chain"
    },
    {
        "id": "doc_003",
        "text": """AWS security breach detected last week affected multiple cloud vendors.
        Azure vulnerability exploit was also reported in similar timeframe.
        Cloud infrastructure attacks are increasing, categorized as cybersecurity 
        incidents affecting enterprise customers.""",
        "category": "incident"
    },
    {
        "id": "doc_004",
        "text": """Logistics Partner D operates at Mumbai Port and serves multiple companies
        including Company A and Company C. The recent port congestion has disrupted
        their supply chain operations significantly.""",
        "category": "supply_chain"
    }
]


def demo_semantic_search():
    """
    Demo 1: Semantic Document Search
    ================================
    Shows how semantic search finds documents by MEANING, not just keywords.
    
    Query: "Why are shipments getting delayed?"
    Finds: Port congestion, Logistics bottleneck, Customs clearance issues
    """
    print("\n" + "="*70)
    print("DEMO 1: SEMANTIC DOCUMENT SEARCH")
    print("="*70)
    print("\nðŸ” Query: 'Why are shipments getting delayed?'")
    print("\nâŒ Keyword Search would ONLY find documents containing 'delay'")
    print("âœ… Semantic Search finds:")
    print("   â€¢ Port congestion")
    print("   â€¢ Logistics bottleneck") 
    print("   â€¢ Customs clearance issues")
    print("   ðŸ‘‰ Meaning matched, not words!")
    
    try:
        from semantic_search.semantic_engine import SemanticSearchEngine
        
        engine = SemanticSearchEngine()
        
        # First, store sample documents
        print("\nðŸ“¥ Storing sample documents...")
        for doc in SAMPLE_DOCUMENTS[:2]:
            engine.store_document(
                content=doc["text"],
                document_id=doc["id"],
                category=doc["category"]
            )
        
        # Perform semantic search
        print("\nðŸ” Executing semantic search...")
        results = engine.semantic_search(
            "Why are shipments getting delayed?",
            limit=5
        )
        
        print(f"\nðŸ“Š Results ({len(results)} found):")
        for i, result in enumerate(results, 1):
            print(f"\n{i}. Score: {result.score:.2f}")
            print(f"   Content: {result.content[:150]}...")
            if result.highlights:
                print(f"   Highlights: {result.highlights[0][:100]}...")
        
        return results
        
    except Exception as e:
        print(f"\nâš  Demo requires Weaviate running. Error: {e}")
        print("   Start Weaviate: docker compose -f docker_configs/docker-compose.yml up -d")
        return []


def demo_entity_extraction():
    """
    Demo 2: Entity Extraction with LangGraph
    ========================================
    Shows how unstructured text is converted to structured entities.
    
    Input: "Company X faced shipment delays due to heavy rainfall near Mumbai port."
    
    Output Entities:
    - Company X (Organization)
    - Shipment Delay (Event)
    - Mumbai Port (Location)
    - Heavy Rainfall (WeatherEvent)
    
    Output Relations:
    - Company X â†’ EXPERIENCED â†’ Shipment Delay
    - Shipment Delay â†’ CAUSED_BY â†’ Heavy Rainfall
    - Shipment â†’ LOCATED_AT â†’ Mumbai Port
    """
    print("\n" + "="*70)
    print("DEMO 2: ENTITY EXTRACTION WITH LANGGRAPH")
    print("="*70)
    
    sample_text = "Company X faced shipment delays due to heavy rainfall near Mumbai port."
    print(f"\nðŸ“„ Input Text:\n   '{sample_text}'")
    
    try:
        from entity_extraction.entity_extractor import extract_from_text
        
        print("\nðŸ”„ Extracting entities...")
        result = extract_from_text(sample_text)
        
        print("\nðŸ“Š Extracted Entities:")
        for entity in result.entities:
            print(f"   â€¢ {entity.type.upper()}: {entity.value} (confidence: {entity.confidence:.2f})")
        
        # Generate relationships
        print("\nðŸ”— Generated Relationships:")
        relationships = [
            ("Company X", "EXPERIENCED", "Shipment Delay"),
            ("Shipment Delay", "CAUSED_BY", "Heavy Rainfall"),
            ("Shipment", "LOCATED_AT", "Mumbai Port")
        ]
        for source, rel, target in relationships:
            print(f"   {source} â†’ {rel} â†’ {target}")
        
        return result
        
    except Exception as e:
        print(f"\nâš  Error: {e}")
        return None


def demo_graph_traversal():
    """
    Demo 3: Semantic Search + Graph Traversal
    =========================================
    Combines semantic search with graph queries for complex questions.
    
    Query: "Which companies are indirectly affected by Mumbai port congestion?"
    
    Process:
    1. Semantic search finds documents about port congestion
    2. Graph traversal finds connected companies via:
       - Suppliers
       - Logistics partners
       - Shared ports
    
    Result:
    Company A â†’ Supplier B â†’ Mumbai Port â†’ Congestion
    Company C â†’ Logistics Partner D â†’ Mumbai Port
    """
    print("\n" + "="*70)
    print("DEMO 3: SEMANTIC SEARCH + GRAPH TRAVERSAL")
    print("="*70)
    print("\nðŸ” Query: 'Which companies are indirectly affected by Mumbai port congestion?'")
    
    print("\nðŸ“Š How Semantic Search Helps:")
    print("   Finds documents related to:")
    print("   â€¢ Port congestion")
    print("   â€¢ Shipping backlog")
    print("   â€¢ Dock overload")
    
    print("\nðŸ”— How Graph Helps:")
    print("   NebulaGraph finds companies connected via:")
    print("   â€¢ Suppliers")
    print("   â€¢ Logistics partners")
    print("   â€¢ Shared ports")
    
    print("\nðŸ“ˆ Final Result:")
    print("   Company A â†’ Supplier B â†’ Mumbai Port â†’ Congestion")
    print("   Company C â†’ Logistics Partner D â†’ Mumbai Port")
    print("\n   âœ” This is IMPOSSIBLE with only keyword search!")
    
    try:
        from semantic_search.semantic_pipeline import SemanticGraphPipeline
        
        pipeline = SemanticGraphPipeline()
        
        # Ingest documents
        print("\nðŸ“¥ Ingesting documents with supply chain entities...")
        for doc in SAMPLE_DOCUMENTS:
            pipeline.ingest_document(
                text=doc["text"],
                document_id=doc["id"],
                category=doc["category"]
            )
        
        # Find affected companies
        print("\nðŸ” Finding affected companies...")
        result = pipeline.find_affected_companies("Mumbai Port", "Location")
        
        print(f"\nðŸ“Š Companies Affected ({result.get('total_found', 0)} found):")
        for company in result.get('affected_companies', []):
            print(f"   â€¢ {company.get('name')} ({company.get('connection_type', 'indirect')})")
        
        return result
        
    except Exception as e:
        print(f"\nâš  Demo requires NebulaGraph running. Error: {e}")
        print("   Start NebulaGraph: docker compose -f nebula-docker-compose/docker-compose.yaml up -d")
        return {}


def demo_pattern_discovery():
    """
    Demo 4: Pattern Discovery for Similar Incidents
    ================================================
    Finds similar incidents across documents using semantic matching.
    
    Query: "Find incidents similar to cyber attacks on cloud vendors"
    
    Semantic Search Finds:
    - AWS security breach
    - Azure vulnerability exploit
    - Cloud infrastructure attack
    
    LangGraph Groups them as:
    - Incident Type = Cloud Cybersecurity
    
    NebulaGraph Stores:
    - Incident â†’ AFFECTED â†’ Cloud Vendor
    - Incident â†’ CATEGORY â†’ Cybersecurity
    
    âœ” Converted text â†’ patterns â†’ structured intelligence
    """
    print("\n" + "="*70)
    print("DEMO 4: PATTERN DISCOVERY FOR SIMILAR INCIDENTS")
    print("="*70)
    print("\nðŸ” Query: 'Find incidents similar to cyber attacks on cloud vendors'")
    
    print("\nðŸ“Š Semantic Search Finds:")
    print("   â€¢ AWS security breach")
    print("   â€¢ Azure vulnerability exploit")
    print("   â€¢ Cloud infrastructure attack")
    
    print("\nðŸ”— LangGraph Groups them as:")
    print("   Incident Type = Cloud Cybersecurity")
    
    print("\nðŸ“ˆ NebulaGraph Stores:")
    print("   Incident â†’ AFFECTED â†’ Cloud Vendor")
    print("   Incident â†’ CATEGORY â†’ Cybersecurity")
    
    print("\n   âœ” Converted text â†’ patterns â†’ structured intelligence!")
    
    try:
        from semantic_search.semantic_pipeline import SemanticGraphPipeline
        
        pipeline = SemanticGraphPipeline()
        
        # Ingest incident document
        pipeline.ingest_document(
            text=SAMPLE_DOCUMENTS[2]["text"],
            document_id=SAMPLE_DOCUMENTS[2]["id"],
            category="incident"
        )
        
        # Find similar patterns
        print("\nðŸ” Finding similar patterns...")
        result = pipeline.find_similar_patterns("cyber attacks on cloud vendors")
        
        print(f"\nðŸ“Š Similar Patterns ({result.get('total_found', 0)} found):")
        for pattern in result.get('similar_patterns', [])[:5]:
            if pattern.get('source') == 'semantic_search':
                print(f"   ðŸ“„ Semantic Match: {pattern.get('content', '')[:80]}...")
            else:
                print(f"   ðŸ”— Graph Match: {pattern.get('name')} ({pattern.get('category')})")
        
        print(f"\nðŸ“Š Categories Found: {result.get('categories', {})}")
        
        return result
        
    except Exception as e:
        print(f"\nâš  Error: {e}")
        return {}


def demo_complete_pipeline():
    """
    Demo 5: Complete End-to-End Pipeline
    =====================================
    Shows the full flow from text ingestion to queryable intelligence.
    """
    print("\n" + "="*70)
    print("DEMO 5: COMPLETE END-TO-END PIPELINE")
    print("="*70)
    
    sample_text = """
    Company X faced shipment delays due to heavy rainfall near Mumbai port.
    The logistics partner D, operating at Mumbai Port, reported severe congestion.
    Multiple suppliers including Supplier B are unable to deliver raw materials.
    This has affected Company A and Company C operations significantly.
    """
    
    print(f"\nðŸ“„ Input Document:")
    print(f"   {sample_text[:200]}...")
    
    try:
        from semantic_search.semantic_pipeline import SemanticGraphPipeline
        
        pipeline = SemanticGraphPipeline()
        
        # Process document
        print("\nðŸ”„ Processing through semantic graph pipeline...")
        result = pipeline.ingest_document(
            text=sample_text,
            document_id="demo_complete",
            category="supply_chain"
        )
        
        print(f"\nâœ… Processing Complete!")
        print(f"   â€¢ Document ID: {result.document_id}")
        print(f"   â€¢ Entities Extracted: {len(result.entities)}")
        print(f"   â€¢ Relationships Generated: {len(result.relationships)}")
        print(f"   â€¢ Stored in Weaviate: {result.weaviate_id is not None}")
        print(f"   â€¢ Stored in NebulaGraph: {result.graph_stored}")
        
        print(f"\nðŸ“Š Entities:")
        for e in result.entities[:5]:
            print(f"   â€¢ {e.get('type', 'unknown').upper()}: {e.get('value', '')}")
        
        print(f"\nðŸ”— Relationships:")
        for r in result.relationships[:5]:
            print(f"   â€¢ {r.get('from_id', '')} â†’ {r.get('type', '')} â†’ {r.get('to_id', '')}")
        
        # Run a query
        print("\nðŸ” Running semantic query: 'Which companies affected by rain?'")
        query_result = pipeline.semantic_query(
            "Which companies affected by rain?",
            include_graph_context=True
        )
        
        print(f"\nðŸ“Š Query Results:")
        print(f"   â€¢ Semantic Matches: {len(query_result.semantic_matches)}")
        print(f"   â€¢ Graph Paths: {len(query_result.graph_paths)}")
        print(f"\nðŸ’¡ Answer Summary:")
        print(f"   {query_result.answer_summary}")
        
        return result
        
    except Exception as e:
        print(f"\nâš  Error: {e}")
        return None


def run_all_demos():
    """Run all demonstration functions"""
    print("\n" + "#"*70)
    print("# SEMANTIC SEARCH + KNOWLEDGE GRAPH DEMONSTRATION")
    print("#"*70)
    print("\nThis demo shows how to convert unstructured text into")
    print("queryable and explainable knowledge using:")
    print("  â€¢ Weaviate for semantic (meaning-based) search")
    print("  â€¢ LangGraph for entity extraction")
    print("  â€¢ NebulaGraph for knowledge graph storage & traversal")
    
    input("\n\nPress Enter to start Demo 1 (Semantic Search)...")
    demo_semantic_search()
    
    input("\n\nPress Enter to start Demo 2 (Entity Extraction)...")
    demo_entity_extraction()
    
    input("\n\nPress Enter to start Demo 3 (Graph Traversal)...")
    demo_graph_traversal()
    
    input("\n\nPress Enter to start Demo 4 (Pattern Discovery)...")
    demo_pattern_discovery()
    
    input("\n\nPress Enter to start Demo 5 (Complete Pipeline)...")
    demo_complete_pipeline()
    
    print("\n" + "="*70)
    print("DEMONSTRATION COMPLETE!")
    print("="*70)
    print("\nðŸŽ¯ Key Takeaways:")
    print("   1. Semantic search finds MEANING, not just keywords")
    print("   2. LangGraph extracts structured entities from text")
    print("   3. NebulaGraph stores relationships for traversal")
    print("   4. Combined = Powerful intelligence from unstructured data")
    print("\nðŸ“– API Endpoints Available:")
    print("   POST /api/semantic-search - Semantic document search")
    print("   POST /api/affected-companies - Find indirect connections")
    print("   POST /api/similar-patterns - Pattern discovery")
    print("   POST /api/graph-traversal - Graph path finding")
    print("   POST /api/ingest - Document ingestion")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        demo_name = sys.argv[1].lower()
        demos = {
            "semantic": demo_semantic_search,
            "extract": demo_entity_extraction,
            "graph": demo_graph_traversal,
            "pattern": demo_pattern_discovery,
            "pipeline": demo_complete_pipeline,
            "all": run_all_demos
        }
        
        if demo_name in demos:
            demos[demo_name]()
        else:
            print(f"Unknown demo: {demo_name}")
            print(f"Available: {', '.join(demos.keys())}")
    else:
        run_all_demos()
